{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d277a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 1.13.1+cu117 with CUDA 1107 (you have 2.0.0+cu118)\n",
      "    Python  3.9.16 (you have 3.9.13)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "The model 'GPTNeoXGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딥러닝은 인공 신경망을 통해 더 복잡한 문제를 해결하는 기술입니다. 머신러닝과 달리, 사람이 수행하는 작업을 수행하는 데 큰 도움을 줄 수 있습니다. 예를 들어, 이미지 인식과 같은 분야에서 뛰어난 성능을 보여주고 있습니다. 최근에는 딥러닝을 활용한 인공지능 서비스도 등장하면서 주목받고 있습니다. \n",
      " 더 자세한 설명: 딥러닝은 머신러닝과는 달리, 사람이 수행하는 작업을 보다 복잡한 알고리즘을 통해 수행합니다. 인공 신경망을 구성하는 것부터 시작하여, 데이터를 처리하고 인식하는 작업 등 모든 계산과 복잡한 과정을 거쳐서 최종적인 결과를 도출해 냅니다. 이러한 이유로 딥러닝은 머신러닝보다 더 높은 성능을 보여주면서도, 더 복잡한 알고리즘에 기초하여 문제를 해결할 수 있습니다. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "MODEL = 'beomi/KoAlpaca-Polyglot-5.8B'\n",
    "QUANT_MODEL = './model'\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(QUANT_MODEL, device=\"cuda:0\", use_triton=False)\n",
    "\n",
    "pipe = pipeline('text-generation', model=model,tokenizer=MODEL,device=0)\n",
    "\n",
    "def ask(x, context='', is_input_full=False):\n",
    "    ans = pipe(\n",
    "        f\"### 질문: {x}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {x}\\n\\n### 답변:\", \n",
    "        do_sample=True, \n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "ask(\"딥러닝이 뭐야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18371ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
